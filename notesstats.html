<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notes on Statistics</title>
    <link rel="stylesheet" href="styles/styles.css">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

    <span><b>Notes on Statistics</b></span>

    <p>
        I will avoid rigor; the emphasis of these notes is on practical statistics. However, it is sometimes helpful for
        me to state definitions in a more mathematically precise form than what is found in most non-mathematical
        sources, because it helps ease an itchy feeling.
    </p>

    <p>
        Most of the information presented here has been taken verbatim from the following sources:
    </p>

    <ul>
        <li>
            Hartmann, K., Krois, J., & Rudolph, A. (2023). <em>Statistics and Geodata Analysis using R</em>.
            <a href="https://www.geo.fu-berlin.de/soga-r" target="_blank">SOGA-R</a>
        </li>
        <li>
            Rudolph, A., Krois, J., & Hartmann, K. (2023). <em>Statistics and Geodata Analysis using Python</em>.
            <a href="https://www.geo.fu-berlin.de/soga-py" target="_blank">SOGA-Py</a>
        </li>
        <li>
            The website <a href="http://www.randomservices.org/random/" target="_blank">Random</a>
        </li>
        <li>
            DeGroot, M. H., & Schervish, M. J. (2012). <em>Probability and Statistics</em> (4th ed.). Addison-Wesley.
        </li>
        <li>
            Diez, D. M., Ã‡etinkaya-Rundel, M., & Barr, C. D. (2019). <em>OpenIntro Statistics</em> (4th ed.). OpenIntro.
        </li>
    </ul>

    <p>These notes are broadly organized as follows:</p>

    <ul>
        <li><a href="#CDFQuantile">Distribution and quantile functions</a></li>
        <li><a href="#Hypotheses">Hypotheses</a></li>
        <li><a href="#ErrorSignificance">Error and Significance Level</a></li>
        <li><a href="#CriticalValuePValue">Critical Value and the p-Value</a></li>
        <li><a href="#OnePopulationMeanKnownSigma">Inference for One Population Mean with Known Sigma</a></li>
        <li><a href="#Ttests" target="_blank">T-tests</a></li>
    </ul>

    <hr>

    <span id="CDFQuantile"><b>Distribution and quantile functions</b></span>

    <p>
        There are several definitions of a quantile for a dataset, and they are not necessarily equivalent.
        This nuance is often overlooked. In many cases, the differences between these definitions are small,
        but when sample values are far apart, such as in the long tail of a distribution, the differences can be
        substantial. We will first focus on the more general, single mathematical notion of a quantile.
    </p>

    <p>
        Suppose that \(X\) is a real-valued random variable. The <i>(cumulative) distribution function</i> (CDF) of
        \(X\) is the function \(F: \mathbb{R} \to [0, 1]\) defined by
        \[
        F(x) = \mathbb{P}(X \le x), \quad x \in \mathbb{R}
        \]
    </p>

    <p>
        For \(p \in (0,1)\), any value \(x\) satisfying \(\mathbb{P}(X < x) \le p\) and \(F(x)=\mathbb{P}(X \le x) \ge
            p\) is called a <i>quantile</i> of order \(p\) for the distribution.
    </p>

    <p>
        The <i>quantile function</i> \(F^{-1}\) of \(X\) is defined by
        \[
        F^{-1}(p) = \min\{x \in \mathbb{R}: F(x) \ge p\}, \quad p \in (0, 1)
        \]
    </p>

    <p>
        In Python, you can compute quantiles and CDF values using the <code>scipy.stats</code> library, which provides
        functions for various probability distributions. For example, for the normal distribution you have:
    </p>

    <pre class="plain"><code>import scipy.stats as stats

mu = 0  
sigma = 1 

x = 1
cdf_value = stats.norm.cdf(x, loc=mu, scale=sigma)

p = 0.7
quantile_value = stats.norm.ppf(p, loc=mu, scale=sigma)</code></pre>

    <hr>

    <span id="Hypotheses"><b>Hypotheses</b></span>

    <p>
        In statistical <i>hypothesis testing</i>, the goal is to determine whether there is sufficient
        evidence to reject a presumed <i>null hypothesis</i> in favor of a conjectured <i>alternative hypothesis</i>.
        The null hypothesis is usually denoted by \(H_0\), while the alternative hypothesis is usually
        denoted by \(H_1\).
    </p>

    <p>
        For example, if the hypothesis test concerns deciding whether a population mean \(\mu\)
        differs from a specified value \(\mu_0\), then the null hypothesis can be expressed as
        \[
        H_0: \mu = \mu_0
        \]
    </p>

    <p>
        and the alternative hypothesis as
        \[
        H_1: \mu \neq \mu_0\text{.}
        \]
    </p>

    <p>Such a hypothesis test is called a <b>two-sided test</b>.</p>

    <p>
        If the hypothesis test is about deciding whether a population mean \(\mu\)
        is smaller than the specified value \(\mu_0\), the alternative hypothesis is expressed as
        \[
        H_1: \mu < \mu_0\text{.} \] </p>

            <p>Such a hypothesis test is called a <b>left-tailed test</b>.</p>

            <p>
                Analogously, we have the <b>right-tailed test</b>, which is about deciding whether a population mean
                \(\mu\) is greater than the specified value \(\mu_0\).
            </p>

            <p>
                We make the decision to reject the null hypothesis in favor of the alternative,
                or to not reject the null hypothesis, based on the observed outcome, say \(\mathbf{x}\), of a random
                experiment.
                We identify an appropriate subset \(R\) of the sample space \(S\) and reject \(H_0\) if and
                only if \(\mathbf{x} \in R\).
                This set \(R\) is known as the <b>rejection region</b> or the <b>critical region</b>. However, we will
                see that this
                critical region is often defined in terms of a <i>test statistic</i>, which maps \(S\) into another set
                \(T\). This allows
                for siginificant data reduction.
            </p>

            <hr>

            <span id="ErrorSignificance"><b>Error and Significance Level</b></span>

            <p>
                Conducting a hypothesis test always implies, that there is a chance of making an incorrect decision.
                There are two types of errors, depending on which of the hypotheses is actually true.
                A <b>type I error</b> occurs when a true null hypothesis is rejected (a <i>false positive</i>),
                while a <b>type II error</b> occurs when a false null hypothesis is not rejected (a <i>false
                    negative</i>).
                Similarly, there are two ways to make a correct decision. The possibilities are summarized in the
                following table:
            </p>

            <p>
                \[
                \begin{array}{|l|c|c|}
                \hline
                \textbf{Decision | State} & H_0 \text{ is true} & H_0 \text{ is false} \\
                \hline
                \text{Do not reject } H_0 & \text{Correct decision} & \text{Type II error} \\
                \hline
                \text{Reject } H_0 & \text{Type I error} & \text{Correct decision} \\
                \hline
                \end{array}
                \]
            </p>

            <p>
                The probability of a type I error is commonly called the
                <b>significance level</b> of the hypothesis test and is denoted by \(\alpha\).
                If \(H_0\) is a composite hypothesis, then it specifies a variety of
                different distributions, and thus there is a set of type I error probabilities. In this
                case, the significance level is the maximum probability of a type I error over
                this set of distributions.
            </p>

            <p>
                The probability of a type II error is denoted by \(\beta\). Generally, there is
                a tradeoff between type I and type II error probabilities. If we reduce the probability
                of a type I error by making the rejection region smaller, we necessarily increase the
                probability of a type II error because the complementary region \(S \setminus R\) becomes larger.
            </p>

            <p>
                If a hypothesis test is performed at a certain significance level \(\alpha\) and the null
                hypothesis is rejected, one may state that the test results are
                <b>statistically significant at the \(\alpha\) level</b>. If the null
                hypothesis is not rejected at significance level \(\alpha\), one may state
                that the test results are <b>not statistically significant at the \(\alpha\) level.</b>
            </p>

            <p><i>More on the power of a test later!</i></p>

            <hr>

            <span id="CriticalValuePValue"><b>Critical Value and the p-Value</b></span>

            <p>
                Under the <b>critical value approach</b>, the test statistic,
                calculated based on the observed data, is compared to a certain critical value.
                If the test statistic is more <i>extreme</i> than the critical value, the null hypothesis
                is rejected; otherwise, it is not.
            </p>

            <p>
                This critical value is computed based on the given significance level \(\alpha\) and
                the probability distribution specified by \(H_0\).
            </p>

            <p>
                Let us consider the simpler case in which \(H_0\) is not composite and the distribution
                it specifies is a bell-shaped normal distribution. The critical value divides the area under
                the probability distribution curve into the rejection region(s) and the non-rejection region.
            </p>

            <p>
                In a two-sided test, the null hypothesis is rejected
                if the test statistic is either too small or too large.
                Thus, the rejection region for such a test consists of two parts: one on the left and one on the right.
            </p>

            <div class="img-div">
                <img src="images/two_sided_critical_value.png" alt="two-sided critical value">
            </div>

            <p>
                For a left-tailed test, the null hypothesis is rejected if the test statistic is too small. Thus, the
                rejection region for such a test consists of one part, which is to the left of the center.
            </p>

            <div class="img-div">
                <img src="images/left_sided_critical_value.png" alt="left-tailed critical value">
            </div>

            <p>
                For a right-tailed test, the null hypothesis is rejected if the test statistic is too large. Thus, the
                rejection region for such a test consists of one part, which is to the right of the center.
            </p>

            <div class="img-div">
                <img src="images/right_sided_critical_value.png" alt="right-tailed critical value">
            </div>

            <p>
                In most cases, we have a general procedure that allows us to
                construct a test (that is, a rejection region \(R_{\alpha}\)) for any given
                significance level \(\alpha \in (0,1)\). Typically, \(R_{\alpha}\) decreases (in the subset sense)
                as \(\alpha\) decreases.
            </p>

            <p>
                The <b>\(\mathbf{p}\)-value</b> of the observed value \(x\) is defined to be the smallest
                \(\alpha\) for which \(x \in R_{\alpha}\), that is, the smallest significance level for which
                \(H_0\) is rejected, given the observed value \(x\).
            </p>

            <p>
                Let us denote this \(p\)-value by \(P(x)\). Knowing \(P(x)\) allows us to test \(H_0\) at
                any significance level for the given data \(x\): if \(P(x) \leq \alpha\), then we
                would reject \(H_0\) at significance level \(\alpha\); if \(P(x) > \alpha\), then we fail
                to reject \(H_0\) at significance level \(\alpha\).
            </p>

            <p>
                Informally, the p-value corresponds to the probability of observing sample data at least as extreme as
                the actually obtained test statistic. Small p-values provide evidence against the null hypothesis. The
                smaller (closer to 0) the p-value, the stronger the evidence against the null hypothesis.
            </p>

            <hr>

            <span id="OnePopulationMeanKnownSigma"><b>Inference for One Population Mean with Known Sigma</b></span>

            <p>
                The simplest form of hypothesis test is the test for one population mean.
                If the population standard deviation \(\sigma\) is known, a hypothesis test for one population mean is
                called <b>one-mean \(\mathbf{z}\)-Test</b> or simply <b>\(\mathbf{z}\)-Test</b>.
            </p>

            <p>
                To perform the \(z\)-test we follow the step-wise procedure shown below. First, we showcase the critical
                value approach. Secondly, we repeat the analysis for the \(p\)-value approach.
            </p>

            <ol>
                <li>State the null hypothesis \(H_0\) and the alternative hypothesis \(H_A\).</li>
                <li>Decide on the significance level, \(\alpha\).</li>
                <li>Compute the value of the test statistic.</li>
                <li>Under the critical value approach, determine the critical value. <br>
                    Under the p-value approach, determine the \(p\)-value.</li>
                <li>
                    Critical value approach: If the value of the test statistic falls in the rejection region, reject
                    \(H_0\); otherwise, do not reject \(H_0\). <br>
                    \(p\)-value approach: If \(p \leq \alpha\), reject \(H_0\); otherwise, do not reject \(H_0\).
                </li>
                <li>Interpret the result of the hypothesis test.</li>
            </ol>

            <p>
                We will work with the <i>students</i> dataset in Python:
            </p>
            <pre class="plain"><code>>>> import pandas as pd
>>> import numpy as np
>>> students = pd.read_csv("https://userpage.fu-berlin.de/soga/data/raw-data/students.csv")</code></pre>

            <p>
                The <i>students</i> dataset contains 8,239 rows, each representing an individual student, and 16
                columns, each corresponding to a variable or feature related to that student:
            </p>
            <pre class="plain"><code>>>> students.head(5)
   stud.id                 name       gender  age  ...  score2  online.tutorial graduated  salary
1   833917  Gonzales, Christina  Female   19  ...     NaN                0         0     NaN
2   898539       Lozano, T'Hani  Female   19  ...     NaN                0         0     NaN
3   379678       Williams, Hanh  Female   22  ...    46.0                0         0     NaN
4   807564          Nem, Denzel    Male   19  ...     NaN                0         0     NaN
5   383291      Powell, Heather  Female   21  ...     NaN                0         0     NaN

[5 rows x 16 columns]</code></pre>

            <p>
                In this analysis, we will examine the average weight of students and compare it to the average weight of
                European adults. It has been reported that the average body mass of the European adult population is
                70.8 kg. Since the standard deviation is not known, we will assume it to be equal to the standard
                deviation of the "weight" variable in the <i>students</i> dataset:
            </p>
            <pre class="plain"><code>>>> mu_0 = 70.8
>>> sigma = np.std(students["weight"])
>>> print(round(sigma, 2))
8.63</code></pre>

            <p>
                Next, we will take a random sample of 14 students:
            </p>
            <pre class="plain"><code>>>> n = 14
>>> sample_weights = students.sample(n, random_state=8)["weight"]
>>> mean_sample = np.mean(sample_weights)
>>> print(round(mean_sample, 2))
69.82</code></pre>

            <p>
                The null hypothesis is \(H_0: \mu = 70.8\), and we have three possible alternative hypotheses:
                \(H_{A_1}: \mu \neq 70.8\), \(H_{A_2}: \mu < 70.8\), and \(H_{A_3}: \mu> 70.8\).
            </p>

            <p>
                Let the significance level be \(\alpha = 0.05\):
            </p>
            <pre class="plain"><code>>>> alpha = 0.05</code></pre>

            <p>
                The test statistic \(z\) is defined as \(z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}}\). Applying this
                formula to our data, we have:
            </p>
            <pre class="plain"><code>>>> z = (mean_sample - mu_0) / (sigma / np.sqrt(n))
>>> print(z)
-0.42404547484536753</code></pre>

            <p>
                To calculate the critical values, we use the <code>norm.ppf()</code> function from the
                <code>scipy</code> package. Since we are testing three alternative hypotheses (\(H_{A_1}\), \(H_{A_2}\),
                and \(H_{A_3}\)), we need to calculate three corresponding critical values:
                \(z_{A_1} = \pm z_{\alpha / 2}\), \(z_{A_2} = -z_\alpha\), and \(z_{A_3} = +z_\alpha\):
            </p>
            <pre class="plain"><code>>>> z_ha1 = norm.ppf(alpha / 2)
>>> z_ha2 = norm.ppf(alpha)
>>> z_ha3 = norm.ppf(1 - alpha)
>>> print(z_ha1, z_ha2, z_ha3)
-1.9599639845400545 -1.6448536269514729 1.6448536269514722</code></pre>

            <p>
                Does the test statistic fall in the rejection region for \(H_{A_1}\)?
            </p>
            <pre class="plain"><code>>>> print(z < z_ha1 or z > np.abs(z_ha1))
False</code></pre>

            <p>
                At the 5% significance level, we fail to reject \(H_0\).
            </p>

            <p>
                Does the test statistic fall in the rejection region for \(H_{A_2}\)?
            </p>
            <pre class="plain"><code>>>> print(z < z_ha2)
False</code></pre>

            <p>
                Again, at the 5% significance level, we fail to reject \(H_0\).
            </p>

            <p>
                Does the test statistic fall in the rejection region for \(H_{A_3}\)?
            </p>
            <pre class="plain"><code>>>> print(z > z_ha3)
False</code></pre>

            <p>
                Once more, at the 5% significance level, we fail to reject \(H_0\).
            </p>

            <p>
                Under the \(p\)-value approach, we still require the test statistic \(z\),
                which we have already calculated. To calculate the \(p\)-value using Python, we apply the
                <code>norm.cdf()</code> function from the scipy package to compute the probability of occurrence for the
                test statistic based on the standard normal distribution. Since we are testing three alternative
                hypotheses (\(H_{A_1}\), \(H_{A_2}\), and \(H_{A_3}\)), we need to calculate three \(p\)-values as well:
            </p>

            <pre class="plain"><code>>>> lower = norm.cdf(z)
>>> upper = 1 - norm.cdf(np.abs(z))
>>> p_value_1 = lower + upper
>>> p_value_2 = norm.cdf(z)
>>> p_value_3 = 1 - norm.cdf(z)
>>> print(p_value_1, p_value_2, p_value_3)
0.6715326491107156 0.3357663245553578 0.6642336754446422</code></pre>

            <p>
                Again, at the 5% significance level, we fail to reject \(H_0\), since:
            </p>

            <pre class="plain"><code>>>> print(p_value_1 <= alpha, p_value_2 <= alpha, p_value_3 <= alpha)
False False False</code></pre>


            <p>
                The primary assumption that we made is that the underlying sampling distribution is normal. Of course,
                in real statistical problems, we are unlikely to know much about the sampling distribution, let alone
                whether or not it is normal. Suppose in fact that the underlying distribution is not normal. When the
                sample size \(n\) is relatively large, the distribution of the sample mean will still be approximately
                normal by the central limit theorem, and thus our tests of the mean \(\mu\) should still be
                approximately
                valid.
            </p>

            <hr>

            <span id="Ttests"><b>T-tests</b></span>

            <p>
                Our starting point for these tests will usually be a random sample
                \( \mathbf{X} = (X_1, X_2, \dots, X_n) \)
                from a normal distribution whose mean \(\mu\) and variance \(\sigma^2\) may or may not be known.
            </p>

            <p>
                The basic <i>test statistics</i>, the sample mean \(M\) and the sample variance \(S^2\), are defined as
                \[
                M = \frac{1}{n} \sum_{i=1}^n X_i, \quad
                S^2 = \frac{1}{n - 1} \sum_{i=1}^n (X_i - M)^2
                \]
            </p>

            <p>
                Both \(M\) and \(S^2\) are <i>unbiased</i> and <i>consistent</i> estimators of \(\mu\) and \(\sigma^2\),
                respectively.
            </p>

            <p>
                From these basic statistics, we can construct other test statistics that are quite useful for hypothesis
                testing, namely:
                \[
                Z = \frac{M - \mu}{\sigma / \sqrt{n}}, \quad
                T = \frac{M - \mu}{S / \sqrt{n}}, \quad
                V = \frac{n - 1}{\sigma^2} S^2
                \]
            </p>

            <p>In the above:</p>

            <ul>
                <li>\(Z\) has the standard normal distribution.</li>
                <li>\(T\) has the Student \(t\)-distribution with \(n - 1\) degrees of freedom.</li>
                <li>\(V\) has the chi-square distribution with \(n - 1\) degrees of freedom.</li>
                <li>\(Z\) and \(V\) are independent.</li>
            </ul>

            <p>
                Let us now consider the <b>one-sample t-test</b>. It is reasonable to use this test when you have a
                single
                sample of data, you know the population mean you want to compare against, the population standard
                deviation is
                unknown, and the data is approximately normally distributed (especially for small samples).
            </p>

            <p>
                We first set the significance level \( \alpha \in (0,1)\). Let \(t_k(p)\) denote the quantile of order
                \(p\)
                for the Student \(t\)-distribution with \(k\) degrees of freedom.
            </p>

            <p><i style="color: red;">Incomplete!</i></p>

</body>

</html>